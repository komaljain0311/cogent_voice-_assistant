# ğŸš€ `app.py` â€“ Main Application (Budger AI Assistant)

The `app.py` file is the **main entry point** for Budger â€” an AI-powered assistant built using FastAPI, GPT-3.5 Turbo(OpenAI), LangChain, ChromaDB, and SQLite. It handles all backend functionality, including chat routing, user authentication, PDF ingestion, and real-time streaming responses.

---

## ğŸ“Œ Key Responsibilities

- ğŸ”§ Initialize FastAPI with middleware and CORS
- ğŸ” Load API key from `.env`
- ğŸ§  Run GPT-3.5 Turbo-powered OptimizedRAGSystem
- ğŸ“„ Handle PDF uploads and vector embeddings
- ğŸ’¬ Enable real-time WebSocket chat
- ğŸ—‚ï¸ Manage users and chat logs via SQLite
- ğŸ” Serve a dynamic chat UI via static files

---

## ğŸ§© Core Technologies

| Component | Purpose |
|----------|---------|
| **FastAPI** | Web API and routing |
| **GPT-3.5 Turbo (OpenAI)** | Language model for response generation |
| **LangChain** | Document loading and chunking |
| **ChromaDB** | Vector database for semantic search |
| **SQLite** | Stores users and chat history |
| **WebSocket** | Real-time streaming of AI responses |
| **chat.html** | Custom ChatGPT-style frontend UI |

---

## ğŸ” Environment Configuration

The OpenAI API key is loaded from the `.env` file and configured using the OpenAI SDK:
```python
from dotenv import load_dotenv
import os
from openai import OpenAI

# Load environment variable
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# Configure OpenAI client
client = OpenAI(api_key=openai_api_key)
```

## ğŸ§  OptimizedRAGSystem

`OptimizedRAGSystem` is a custom class that powers Budger's **Retrieval-Augmented Generation (RAG)** pipeline. It integrates document search with OpenAIsâ€™s LLM to generate smart, context-aware responses.

This class lives inside `app.py` and handles the core AI logic for both chat and streaming endpoints.

---

### ğŸ” Key Responsibilities

| Component | Description |
|----------|-------------|
| ğŸ“„ `load_documents()` | Loads and parses documents from PDF |
| âœ‚ï¸ `split_documents()` | Splits content into smaller, overlapping text chunks |
| ğŸ§  `embed_documents()` | Converts chunks into vectors using OpenAI embeddings |
| ğŸ” `query_documents()` | Performs similarity search in ChromaDB |
| ğŸ’¬ `generate_response()` | Sends prompt + context to OpenAI |
| ğŸ” `stream_response()` | Streams response back word-by-word |
| ğŸ§  `memory` | Maintains session-level history (chat memory) |

---

### ğŸ› ï¸ How It Works

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

self.llm = ChatOpenAI(
    temperature=0.3,
    streaming=True,
    model="gpt-3.5-turbo",
    api_key=openai_api_key
)
self.embeddings = OpenAIEmbeddings(api_key=openai_api_key)
self.vectorstore = Chroma(
    persist_directory="enhanced_chroma_store",
    embedding_function=self.embeddings
)
```


## ğŸŒ API Endpoints

The following API routes are handled in `app.py`:

| Method     | Path                    | Description                               |
|------------|-------------------------|-------------------------------------------|
| `GET`      | `/`                     | Loads login page (`login.html`)           |
| `GET`      | `/home`                 | Loads chat UI (`chat.html`)               |
| `GET`      | `/health`               | Returns model and server status    |
| `POST`     | `/chat`                 | Returns full GPT-generated response    |
| `POST`     | `/chat/stream`          | Streams GPT response word-by-word       |
| `POST`     | `/upload`               | Uploads PDF and stores in ChromaDB        |
| `POST`     | `/signup`               | Creates a new user in SQLite              |
| `POST`     | `/login`                | Validates credentials                     |
| `DELETE`   | `/sessions/{session_id}`| Clears memory for a session               |
| `WebSocket`| `/ws/{session_id}`      | Enables real-time two-way chat            |

---

âœ… These routes power the complete functionality of Budger's backend â€” including login, document ingestion, AI chat, and live streaming.

---

## ğŸ” WebSocket Streaming

Budger uses a WebSocket endpoint to support **real-time AI conversation**, streaming GPT-3.5 Turbo responses word-by-word like ChatGPT.

```python
@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    # Connect and receive user message
    # Process with OptimizedRAGSystem
    # Stream back OpenAI's reply word-by-word
```


## ğŸ§¾ SQLite Database

The backend uses a local SQLite database (`budger_users.db`) to manage user credentials and chat history.

---

### ğŸ—‚ï¸ `users` Table

| Field      | Description                      |
|------------|----------------------------------|
| `id`       | Auto-incremented user ID         |
| `username` | Unique username for login        |
| `email`    | User's email address             |
| `password` | Raw password (âš ï¸ should be hashed in production) |

---

### ğŸ—ƒï¸ `chat_history` Table

| Field        | Description                                 |
|--------------|---------------------------------------------|
| `user_id`    | Foreign key linked to `users.id`            |
| `session_id` | Unique session ID for each conversation     |
| `query`      | The user's message/question                 |
| `response`   | The AI assistantâ€™s answer (from GPT)     |
| `timestamp`  | Time of the interaction (auto-filled)       |

---

â„¹ï¸ The table schemas are created by `setup_db.py` and used by `DatabaseManager` in `app.py` for inserting and retrieving data.

---

## ğŸ“ Static Files and Frontend

Budger serves a simple web-based chat UI using static files located in the `/static` directory. These are rendered directly via FastAPI routes.

---

### ğŸ“‚ Files and Their Purpose

| File         | Description                              |
|--------------|------------------------------------------|
| `login.html` | Login screen shown at `/`                |
| `chat.html`  | Main chat interface shown at `/home`     |
| `script.js`  | Handles WebSocket connection and messages|
| `style.css`  | Styles the chat UI                       |

---

### âš™ï¸ Static Mounting in FastAPI

```python
from fastapi.staticfiles import StaticFiles
app.mount("/static", StaticFiles(directory="static"), name="static")
```
---

## ğŸ’» How to Run the App

Follow these steps to launch the Budger assistant:

1. Make sure your `.env` file exists and contains your Gemini API key:

```env
OPENAI_API_KEY=your_openai_api_key_here
```

2. Run the FastAPI app:
```
python app.py
```

3.Open your browser and go to:

http://localhost:8000

4. Sign up or log in with your credentials to start chatting with the AI.
